{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOiVrEdZi+wglsBq7vWr+A6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashokteja123/Machine-Learning/blob/main/ID3_Algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Import Necessary Libraries**"
      ],
      "metadata": {
        "id": "IQHTCmJr7-5k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " importing the required libraries for data handling and visualization.\n",
        "\n"
      ],
      "metadata": {
        "id": "M3km5XdV7-4h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gQfFNQ8-5U8V"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from math import log2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Define Functions for Entropy and Information Gain\n",
        "bold text bold text"
      ],
      "metadata": {
        "id": "RKkjlB1W9Bw1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.\n",
        "Entropy**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ">\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " Measures the impurity or disorder in a dataset. A pure dataset, where all instances belong to the same class, has an entropy of 0, while a dataset with an equal distribution among classes has the highest entropy.\n",
        "\n",
        "\n",
        "\n",
        "Formula:\n",
        "          \n",
        "H(S)=\n",
        "              i=1\n",
        "∑\n",
        "c\n",
        "​\n",
        " P(i)log\n",
        "2\n",
        "​\n",
        " P(i)  \n",
        "\n",
        " where\n",
        " H(S) = entropy of the dataset S\n",
        "\n",
        "\n",
        "c = number of classes in the dataset\n",
        "P\n",
        "(\n",
        "i\n",
        ")\n",
        "\n",
        "             P(i) = proportion of instances in class i\n",
        "\n",
        "2. Calculating Proportions\n",
        "For a dataset with\n",
        "n\n",
        " instances and​\n",
        "  instances of class\n",
        "   \n",
        "           p(k)= nK/n\n",
        "​\n",
        "\n",
        "​\n",
        "\n"
      ],
      "metadata": {
        "id": "6alzyx1R9JVF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Information Gain**\n",
        "\n",
        ">\n",
        "\n",
        "\n",
        "Information Gain measures the reduction in entropy after splitting the dataset based on an attribute. It helps identify the attribute that provides the most significant increase in classification accuracy.\n",
        "\n",
        "\n",
        "\n",
        "Formula:  \n",
        "\n",
        "                 IG(S,A)=H(S)−H(SA)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "A = the attribute being considered for the split\n",
        "\n",
        "\n",
        "\n",
        "H(S\n",
        "A\n",
        "​\n",
        " )   is the weighted entropy after splitting the dataset\n",
        "S\n",
        "S based on attribute\n",
        "A\n",
        "A\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Where:\n",
        ": Original dataset.\n",
        ": Attribute being evaluated.\n",
        "​: Subset of  where attribute  takes value .\n",
        "Example: If splitting the dataset by an attribute reduces the overall entropy from 0.97 to 0.58, the information gain is:\n",
        "0.39\n"
      ],
      "metadata": {
        "id": "9W0seMxv_KH3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **ID3 Algorithm Role**\n",
        "\n",
        "\n",
        "Calculate Entropy: Compute the entropy for the dataset.\n",
        "\n",
        "Evaluate Attributes: Compute the information gain for each attribute.\n",
        "\n",
        "Select Attribute: The attribute with the highest information gain becomes the decision node"
      ],
      "metadata": {
        "id": "Az_tollx_5aw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entropy Calculation:\n",
        "defining functions for entropy and info gain"
      ],
      "metadata": {
        "id": "7Xyg_4Q-AfOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_entropy(data):\n",
        "    labels = data.iloc[:, -1].value_counts()\n",
        "    total = len(data)\n",
        "    entropy = -sum((count / total) * log2(count / total) for count in labels)\n",
        "    return entropy"
      ],
      "metadata": {
        "id": "-BqYJOnO9HxF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_information_gain(data, attribute):\n",
        "    total_entropy = calculate_entropy(data)\n",
        "    values = data[attribute].unique()\n",
        "    weighted_entropy = 0\n",
        "\n",
        "    for value in values:\n",
        "        subset = data[data[attribute] == value]\n",
        "        weighted_entropy += (len(subset) / len(data)) * calculate_entropy(subset)\n",
        "\n",
        "    return total_entropy - weighted_entropy"
      ],
      "metadata": {
        "id": "YmGnNwSWA5Vk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building id3 algorithm"
      ],
      "metadata": {
        "id": "cRTpgtXaBuCb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EM8UOkPWBuBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def id3(data, features):\n",
        "    if len(data.iloc[:, -1].unique()) == 1:\n",
        "        return data.iloc[0, -1]\n",
        "\n",
        "    if len(features) == 0:\n",
        "        return data.iloc[:, -1].mode()[0]\n",
        "\n",
        "    gains = {feature: calculate_information_gain(data, feature) for feature in features}\n",
        "    best_feature = max(gains, key=gains.get)\n",
        "\n",
        "    tree = {best_feature: {}}\n",
        "    for value in data[best_feature].unique():\n",
        "        subset = data[data[best_feature] == value]\n",
        "        remaining_features = [feat for feat in features if feat != best_feature]\n",
        "        tree[best_feature][value] = id3(subset, remaining_features)\n",
        "\n",
        "    return tree"
      ],
      "metadata": {
        "id": "3p4QmlfgA5UI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "example dataset"
      ],
      "metadata": {
        "id": "wOWGKoMNB44u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame({\n",
        "    'Outlook': ['Sunny', 'Sunny', 'Overcast', 'Rainy', 'Rainy'],\n",
        "    'Temperature': ['Hot', 'Hot', 'Hot', 'Mild', 'Cool'],\n",
        "    'Humidity': ['High', 'High', 'High', 'Normal', 'Normal'],\n",
        "    'Wind': ['Weak', 'Strong', 'Weak', 'Weak', 'Weak'],\n",
        "    'PlayTennis': ['No', 'No', 'Yes', 'Yes', 'Yes']\n",
        "})\n",
        "\n",
        "features = list(data.columns[:-1])\n",
        "tree = id3(data, features)\n",
        "print(tree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dP_KquCZBm0G",
        "outputId": "41eb0bc1-c0f4-4c9f-988b-ee5c04cf2eaf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Outlook': {'Sunny': 'No', 'Overcast': 'Yes', 'Rainy': 'Yes'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 5:Visualize the decision tree\n",
        "  To understand, and visualize the decision tree using libraries like Graphviz.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "3U7A0nrVCZ1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install graphviz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmx5SyyUCoOB",
        "outputId": "6b7406eb-0d9c-4476-d192-37af901842a2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (0.20.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from graphviz import Digraph\n",
        "\n",
        "def visualize_tree(tree, parent=None, graph=None):\n",
        "    if graph is None:\n",
        "        graph = Digraph()\n",
        "\n",
        "    for key, value in tree.items():\n",
        "        if isinstance(value, dict):\n",
        "            graph.node(key, key)\n",
        "            for sub_key in value:\n",
        "                graph.edge(key, sub_key)\n",
        "                visualize_tree({sub_key: value[sub_key]}, key, graph)\n",
        "        else:\n",
        "            graph.node(value, value)\n",
        "            graph.edge(parent, value)\n",
        "    return graph\n",
        "\n",
        "visualize_tree(tree).view()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "p055VzWoC3DH",
        "outputId": "a85f0996-9777-4a0e-ac13-c973de2c00c6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Digraph.gv.pdf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Advantages of the ID3 Algorithm\n",
        "\n",
        "1. **Simplicity and Interpretability**: Produces easy-to-understand decision trees, making them accessible to non-technical users.\n",
        "\n",
        "2. **Efficient with Categorical Data**: Handles categorical attributes well without needing extra preprocessing.\n",
        "\n",
        "3. **Greedy Approach**: Quickly constructs trees by selecting attributes with the highest information gain.\n",
        "\n",
        "4. **Foundation for Advanced Algorithms**: Serves as a basis for more advanced decision tree algorithms like C4.5 and C5.0.\n",
        "\n",
        "5. **Versatile Applications**: Applicable in various fields, including healthcare, finance, and marketing.\n",
        "\n",
        "### Limitations of the ID3 Algorithm\n",
        "\n",
        "1. **Overfitting**: Can create overly complex trees that fit noise in the training data, affecting generalization.\n",
        "\n",
        "2. **Challenges with Continuous Data**: Struggles with continuous attributes, which need to be discretized, potentially losing information.\n",
        "\n",
        "3. **Bias Towards Multi-Valued Attributes**: Favors attributes with many unique values, which may not always be the most relevant.\n",
        "\n",
        "4. **No Pruning**: Lacks mechanisms to simplify trees, which can lead to overly large and complex models.\n",
        "\n",
        "5. **Scalability Issues**: Faces difficulties with large datasets due to high computational and memory requirements.\n",
        "\n"
      ],
      "metadata": {
        "id": "xnxhBzRzEvNY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RQWcb6pDCZzm"
      }
    }
  ]
}